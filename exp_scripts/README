Regarding exp_script/DARPAscript.py and DARPA.conf.sample:

In order to make it work you need to install the Antoine version of the
liblinear library (to do very very fast SVM learning). Antoine added the
code in the preprocessor archive. In the first line of the script, a path
to the lib/liblinear folder is hard coded, you should change it to yours.

Also, you need to have jobman. And in your PYTHONPATH you should put
the deepANN path.

Then all the parameters needed to launch the experiments are in
DARPA.conf. They are currently at the values of our best setting so far.

The script does greedy layer-wise training on GPUs or CPU. Testing is
performed periodically. There is a list of epochs (that you give as
parameter 'epochstest' in the conf file), one per layer, that specifies
which this testing is performed. Each time we perform testing, we evaluate an unsupervised and supervised criterion over test:
    * Unsupervised: we calculate the reconstruction error on the first
    25000 examples of our test set (to be sure not to overfit).

    * Supervised: We train linear SVMs on the current layer representation
    with different numbers of labeled training examples:
        - 100 training examples (x20 different samples of 100 training examples)
        - 1000 training examples (x10 different samples of 1000 training examples)
        - 10000 training examples (x1 different sample of 10000 training examples)

    We evaluate these supervised models on the 'name_testdata' validation
    instances.

    For each different number of training examples, we also try different
    SVM C values, varying by a factor of 10. We evaluate the C value on
    the validation instances. We take the mean validation score over the
    different samples used at this # of training examples. We choose
    the C value that minimizes this mean validation score, and report
    this mean validation score.

    This entire supervised evaluation appears to be a time-consuming:
        (20 * 100 examples, 10 * 1000 examples, 1* 1000 examples) *
        number of C visited (at least 3)
    but in fact it takes about 30 minutes (thanks to Antoine and liblinear
    :). In principle, we could train the SVMs in parallel on a CPU with a
    thread (I've already done it to generate shapeset data on CPU while
    training on GPU). At the moment it is not implemented.

    For huge models (5000 hidden units per layer) the SVM might take up
    to 2 GB of RAM on the CPU. This model size fits also on the GPUs we
    tested (GTX480).

You can set an experiment that does all the layers in one job, or just
add some layer to a previous trained model (you can give in the parameters
the path of the model you want to load).

I am currently testing the script to ensure it works well, at least now
it seems to.

You should use the GPU for this size of inputs to have results in approx
8h for 3 layers in our best setting, instead of 5 days with CPU.

The simple command to lauch the script properly is:

THEANO_FLAGS=mode=FAST_RUN,device=gpu0*(or another gpu device
number)*,floatX=float32 jobman cmdline DARPAscript.NLPSDAE DARPA.conf

Don't forget that we still need RAM on the CPU (around 2Gig in the worst
case).
